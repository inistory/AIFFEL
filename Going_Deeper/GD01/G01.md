

# G1 텍스트 데이터 다루기
 
## 1. 희소 표현 (Sparse representation)
- 분산 표현과 반대되는 표현
- 단어의 의미를 표현하는 접근 방식에서 분산표현과 차이 존재
- **희소 표현**방식은 벡터의 각 차원마다 단어의 특정 의미 속성을 대응시키는 방식
- 남자: [-1], 여자: [1]
- 소년: [-1, -1], 소녀: [1, -1]
- 할아버지: [-1, 1], 할머니: [1, 1]
- 문제점: 희소 표현의 워드 벡터끼리는 단어들 간의 의미적 유사도를 계산할 수 없음

## 2. 분산표현 (distributed representation)
- 임베딩 레이어를 통해 단어의 분산 표현을 구현 가능
- **Embedding 레이어**를 사용해 **각 단어가 몇 차원의 속성을 가질지 정의**하는 방식
- 희소 표현처럼 속성값을 임의로 지정해 주는 것이 아니라, 수많은 텍스트 데이터를 읽어가며 적합한 값을 찾아감
-  적절히 훈련된 분산 표현 모델을 통해 우리는 단어 간의 의미 유사도를 계산하거나, 이를 feature로 삼아 복잡한 자연어처리 모델을 훈련시킴

## 3. 토큰화
- 문장을 어떤 기준으로 쪼개었을 때, 쪼개진 각 단어들을 **토큰(Token)** 이라고 함
1) 공백 기반 토큰화
- 영어에서 공백기반으로 단어 나누기
- 문장 부호 옆에 공백을 추가
2)  형태소 기반 토큰화
- 한국어 문장은 **공백 기준**으로 토큰화하면 안됨
- 형태소 기준으로 토큰화하기
- 한국어 형태소 분석기
	- KoNLPy : 내부적으로 5가지의 형태소 분석 Class를 포함
	- kakao/khaiii
- 형태소 분석기 특징
		- 속도 측면에서 가장 뛰어난 분석기 : mecab
		- 정확도 : KOMORAN + (mecab, 꼬꼬마)

3) 사전에 없는 단어의 문제
- 토큰화 기법들은 모두 **의미를 가지는 단위로 토큰을 생성**
- 이 기법의 경우, 데이터에 포함되는 모든 단어를 처리할 수는 없기 때문에 자주 등장한 상위 N개의 단어만 사용하고 나머지는 `<unk>`같은 **특수한 토큰(Unknown Token)으로 치환**

4) **OOV(Out-Of-Vocabulary)**문제
- **새로 등장한(본 적 없는) 단어에 대해 약한 모습**을 보일 수밖에 없는 기법
- 해결 방법 : **_Wordpiece Model_**


## 4. 단어의 조각들, Wordpiece Model
- 한 단어를 여러 개의 Subword의 집합으로 보는 방법이 WPM
### **Byte Pair Encoding(BPE)**
- 데이터 압축을 위해서 생김
- **가장 많이 등장하는 바이트 쌍(Byte Pair)** 을 새로운 단어로 치환하여 압축하는 작업을 반복하는 방식으로 동작
- 이를 토큰화에 적용
- 모든 단어를 문자(바이트)들의 집합으로 취급하여 자주 등장하는 문자 쌍을 합치면, 접두어나 접미어의 의미를 캐치할 수 있고, 처음 등장하는 단어는 문자(알파벳)들의 조합으로 나타내어 OOV 문제를 해결
- https://arxiv.org/pdf/1508.07909.pdf
- Embedding 레이어는 **단어의 개수 x Embedding 차원 수** 의 Weight를 생성하기 때문에 단어의 개수가 줄어드는 것은 곧 메모리의 절약으로 이어짐
- 한계 존재

### Wordpiece Model(WPM)
- 구글에서 BPE를 변형해 제안한 알고리즘
- WPM은 BPE에 대해 **두 가지 차별성**
	-   공백 복원을 위해 단어의 시작 부분에 언더바 _ 를 추가
	-  빈도수 기반이 아닌 가능도(Likelihood)를 증가시키는 방향으로 문자 쌍을 합침
	-  `[_i, _am, _a, _b, o, y, _a, n, d, _you, _are, _a, _gir, l]`
-   조사, 어미 등의 활용이 많고 복잡한 한국어 같은 모델의 토크나이저로 WPM이 좋은 대안이 될 수 있다.
-   WPM은 어떤 언어든 무관하게 적용 가능한 language-neutral하고 general한 기법이다. 한국어 형태소 분석기처럼 한국어에만 적용 가능한 기법보다 훨씬 활용도가 크다.

## 5. 토큰에게 의미를 부여하기
### Word2Vec
- _Word2Vec_은 "단어를 벡터로 만든다"는 뜻
- **동시에 등장하는 단어끼리는 연관성이 있다**는 아이디어로 시작된 알고리즘
	- `술`과 `마셨어`
- 은닉층이 1개, 딥러닝 모델이 아님
- Word2Vec에는 두 가지 방식
	- CBOW
	- Skip-gram :성능이 더 좋음

### FastText
- Word2Vec는 연산의 빈부격차가 존재, 자주 등장하지 않는 단어는 최악의 경우 단 한 번의 연산만을 거쳐 랜덤하게 초기화된 값과 크게 다르지 않은 상태로 알고리즘이 종료될 수 있음
- 임베딩 방식 : 한 단어를 n-gram의 집합이라고 보고 단어를 쪼개어 각 n-gram에 할당된 Embedding의 평균값을 사용

### ELMo - the 1st Contextualized Word Embedding
- 위에 소개했던 Word Embedding 알고리즘들은 동음이의어를 처리할 수 없음
	-   이렇게나 탐스럽고 먹음직스러운  **_사과_**를 보셨나요?
	-   저의 간절한  **_사과_**를 받아주시기 바랍니다.
- Word2Vec이든 FastText이든 간에 이 두 문장에 나오는 **사과**의 워드 벡터값은 동일할 수밖에 없음
- 자연어를 이해하려면 문맥(context)의 활용이 필수적입니다.
- '사과'의 context
	- 첫 문장이라면 `탐스럽고 먹음직스러운` 이 될 것이고 다음 문장이라면 `간절한` 이 될 것
	- 즉, 단어의 의미 벡터를 구하기 위해서는 그 단어만 필요한 것이 아니라 그 단어가 놓인 주변 단어 배치의 맥락이 함께 고려되는 Word Embedding이 필요
	- 이런 개념을 `Contextualized Word Embedding`이라고 함

- 2018년 NLP계에 큰 폭풍을 몰고 왔던 _ELMo_라는 모델은 데이터에 단어가 등장한 순간, 그 주변 단어 정보를 사용해 Embedding을 구축하는 개념을 처음 소개하면서 자연어처리의 획기적인 발전의 계기를 마련해 준 첫 번째 `Contextualized Word Embedding` 모델
- ELMo 모델에서 Contextual Word Embedding이 되는 벡터는 어떤 벡터 3가지를 합쳐서 얻어짐
	- 기존 어휘 임베딩(입력 토큰의 word vector), 순방향 LSTM의 hidden state vector, 역방향 LSTM의 hidden state vector를 concatenate한 벡터가 ELMo의 Contextual Word Embedding이 됨
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEyNDM0Njk5NDAsLTMwNjg5NTQ5NSw0MT
QwODczODUsLTkxMDk3Njg4OCw3MzA5OTgxMTZdfQ==
-->